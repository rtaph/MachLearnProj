---
title: "Practical Machine Learning"
author: "rtaph"
date: "December 16, 2014"
---

******************************
## Title
  
The goal of this project is to predict the manner in which they did the exercise. 
"dataset with 5 classes (sitting-down, standing-up, standing, walking, and sitting) collected on 8 hours of activities of 4 healthy subjects." 


### Data Processing
I begin the analysis by loading libraries and setting a few global parameters:
  
```{r chunkOpts, message = FALSE}
  # load needed libraries, set global options, and working directory
  library(knitr); library(caret); library(randomForest)
  library(doMC); registerDoMC(cores = 2)
  opts_chunk$set(echo = TRUE)       
  setwd("~/Documents/Courses/datasciencecoursera/MachLearnProj/")
```

I then check that the data files exist, download it (if needed), and unzip it:

```{r chuckDownload, echo = FALSE}
  # Download file if it does not exist
  if (!file.exists("pml-training.csv")) {
    fileURL <- "http://bit.ly/1GMwAry"
    download.file(fileURL, destfile = "pml-training.csv", method="curl")}
  
  if (!file.exists("pml-testing.csv")) {
    fileURL <- "http://bit.ly/1znCGyy"
    download.file(fileURL, destfile = "pml-testing.csv", method="curl")}
```

A review of the testing dataset reveals that there are only 58 predictor variables. This number is far lower than the training set, which has `r 154-60` extraneous variables. To train the model, we select the feature space that exists both in the training and testing set:

```{r chunkLoadData, cache = TRUE}
  # Read data
  training    =  read.csv("pml-training.csv", na.strings = c("#DIV/0!","NA"))
  testing    =  read.csv("pml-testing.csv", na.strings = c("#DIV/0!","NA"))

  # identify which parameters should be dropped (based on NAs)
  p1 = apply(training, 2, function(x) length(which(!is.na(x))))
  p2 = apply(testing, 2, function(x) length(which(!is.na(x))))
  drop <- p1 == 0 | p2 == 0; rm(p1, p2)
  drop[1] = TRUE

  # keep only data frame containing some data
  training  = training[,!drop]
  testing   = testing[,!drop]
```


With 159 predictor variables available, a model selection method such as best subset selection is unfeasible. 

```{r chunkSubsets, cache = TRUE, autodep=TRUE}
  # smaller subset subset to improve speed
  set.seed(3322)
  s100  = training[sample(nrow(training), 100),]
  s200  = training[sample(nrow(training), 200),]
  s300  = training[sample(nrow(training), 300),]
  s500  = training[sample(nrow(training), 500),]
  s1000 = training[sample(nrow(training), 1000),]
  s1500 = training[sample(nrow(training), 1500),]
  s2000 = training[sample(nrow(training), 2000),]
  s3000 = training[sample(nrow(training), 3000),]
```

```{r chunkRF3000, cache = TRUE, autodep=TRUE}
  # Random Forest Model with 3000 observations
  set.seed(675)
  rf3000 <- train(classe~., data = s3000, method = "rf", prox = TRUE)
```

```{r chunkGBM3000, cache = TRUE, autodep=TRUE}
  # Boosted tree model
  set.seed(981)
  gbm100 <- train(classe~., data = s3000, method = "gbm", verbose = FALSE)
```

```{r chunkResults, cache = TRUE, autodep=TRUE}
  rf3000$finalModel
  gbm100
```

```{r chunkST1, cache = TRUE, autodep=TRUE}
  # compare performancce
  perf = rbind(system.time(train(classe~., data = s100, method = "rf", prox = TRUE)),
        system.time(train(classe~., data = s200, method = "rf", prox = TRUE)),
        system.time(train(classe~., data = s300, method = "rf", prox = TRUE)),
        system.time(train(classe~., data = s500, method = "rf", prox = TRUE)),
        system.time(train(classe~., data = s1000, method = "rf", prox = TRUE)),
        system.time(train(classe~., data = s2000, method = "rf", prox = TRUE)))

  perf = cbind(n = c(100,200,300,500,1000,2000), perf)
  perf
```

```{r chunkST1chart, cache = TRUE, autodep=TRUE}
  # plot performancce
  perf = as.data.frame(perf)
  plot(elapsed~n, data = perf, type = "l")
  lm(elapsed~n, data = perf)$coef[2]*1000/60
```

```{r chunkPreds, cache = TRUE, eval=FALSE}
  pred <- predict(fit.rf, testing); testing$predRight <- pred==testing$classe
  table(pred,testing$classe)
```
