---
title: "Practical Machine Learning"
author: "rtaph"
date: "December 16, 2014"
---

******************************
## Title
  
The goal of this project is to predict the manner in which they did the exercise. 
"dataset with 5 classes (sitting-down, standing-up, standing, walking, and sitting) collected on 8 hours of activities of 4 healthy subjects." 


### Data Processing
I begin the analysis by loading libraries and setting a few global parameters:
  
```{r chunkOpts, cache = TRUE, message = FALSE}
  # load needed libraries, set global options, and working directory
  library(knitr); library(caret); library(randomForest)
  library(doMC); registerDoMC(cores = 2)
  opts_chunk$set(echo = TRUE)       
  setwd("~/Documents/Courses/datasciencecoursera/MachLearnProj/")
```

I then check that the data files exist, download it (if needed), and unzip it:

```{r chuckDownload, echo = FALSE}
  # Download file if it does not exist
  if (!file.exists("pml-training.csv")) {
    fileURL <- "http://bit.ly/1GMwAry"
    download.file(fileURL, destfile = "pml-training.csv", method="curl")}
  
  if (!file.exists("pml-testing.csv")) {
    fileURL <- "http://bit.ly/1znCGyy"
    download.file(fileURL, destfile = "pml-testing.csv", method="curl")}
```

A review of the testing dataset reveals that there are only 58 predictor variables. This number is far lower than the training set, which has `r 154-60` extraneous variables. To train the model, we select the feature space that exists both in the training and testing set:

```{r chunkLoadData, cache = TRUE}
  # Read data
  training    =  read.csv("pml-training.csv", na.strings = c("#DIV/0!","NA"))
  testing    =  read.csv("pml-testing.csv", na.strings = c("#DIV/0!","NA"))

  # identify which parameters should be dropped (based on NAs)
  p1 = apply(training, 2, function(x) length(which(!is.na(x))))
  p2 = apply(testing, 2, function(x) length(which(!is.na(x))))
  drop <- p1 == 0 | p2 == 0; rm(p1, p2)
  drop[1] = TRUE

  # keep only data frame containing some data
  training  = training[,!drop]
  testing   = testing[,!drop]
```

```{r chunkSubset, cache = TRUE}
  # smaller subset subset to improve speed
  set.seed(3322); i = training[sample(nrow(training), 2000),]
```

With 159 predictor variables available, a model selection method such as best subset selection is unfeasible. 


```{r chunkRandomForrest, cache = TRUE}
  # Random Forest Model
  set.seed(4811)
  fit.rf <- train(classe~., data = i, method = "rf", prox = TRUE)
```

```{r chunkGBM, cache = TRUE}
  # Boosted tree model
  set.seed(981)
  fit.gbm <- train(classe~., data = i, method = "gbm", verbose = FALSE)
```

```{r chunkResults, cache = TRUE}
  fit.rf
  fit.gbm
```


```{r chunkPreds, cache = TRUE, eval=FALSE}
  pred <- predict(fit.rf, testing); testing$predRight <- pred==testing$classe
  table(pred,testing$classe)
```
