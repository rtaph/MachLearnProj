---
title: "Practical Machine Learning"
author: "rtaph"
date: "December 16, 2014"
---

******************************
## Title
  
The goal of this project is to predict the manner in which they did the exercise. 
"dataset with 5 classes (sitting-down, standing-up, standing, walking, and sitting) collected on 8 hours of activities of 4 healthy subjects." 

Data is taken from [here](http://groupware.les.inf.puc-rio.br/har).


### Data Processing
I begin the analysis by loading libraries and setting a few global parameters:
  
```{r chunkOpts, message = FALSE}
  # load needed libraries, set global options, and working directory
  library(knitr); library(caret); library(randomForest)
  library(doMC); registerDoMC(cores = 2)
  opts_chunk$set(echo = TRUE)       
  setwd("~/Documents/Courses/datasciencecoursera/MachLearnProj/")
```

I then check that the data files exist, download it (if needed), and unzip it:

```{r chuckDownload, echo = FALSE}
  # Download file if it does not exist
  if (!file.exists("pml-training.csv")) {
    fileURL <- "http://bit.ly/1GMwAry"
    download.file(fileURL, destfile = "pml-training.csv", method="curl")}
  
  if (!file.exists("pml-testing.csv")) {
    fileURL <- "http://bit.ly/1znCGyy"
    download.file(fileURL, destfile = "pml-testing.csv", method="curl")}
```

A review of the testing dataset reveals that there are only 58 predictor variables. This number is far lower than the training set, which has 58 extraneous variables. To train the model, we select the feature space that exists both in the training and testing set:

```{r chunkLoadData, cache = TRUE}
  # Read data
  training    =  read.csv("pml-training.csv", na.strings = c("#DIV/0!","NA"))
  testing    =  read.csv("pml-testing.csv", na.strings = c("#DIV/0!","NA"))

  # identify which parameters should be dropped (based on NAs)
  p1 = apply(training, 2, function(x) length(which(!is.na(x))))
  p2 = apply(testing, 2, function(x) length(which(!is.na(x))))
  drop <- p1 == 0 | p2 == 0; rm(p1, p2)
  drop[1] = TRUE

  # keep only data frame containing some data
  training  = training[,!drop]
  testing   = testing[,!drop]
```


With 159 predictor variables available, a model selection method such as best subset selection is unfeasible. 

```{r chunkSub, cache = TRUE, autodep=TRUE}
  # smaller subset subset to improve speed
  set.seed(3322); s5000 = training[sample(nrow(training), 5000),]
```

Now we train different models. I have chosen two statistical learning methods that generally are quite competitive: random forersts and gradient boosting. We train on both these models using 5000 randomly selected observations from the data. The seed is set identically in each case so that model diagnostics may fairly be compare.

```{r chunkRF5000, cache = TRUE, autodep=TRUE}
  # Random forest model with 5000 observations
  set.seed(675)
  rf5000 <- train(classe~., data = s5000, method = "rf", prox = TRUE)
```

```{r chunkGBM5000, cache = TRUE, autodep=TRUE}
  # Boosted tree model
  set.seed(675)
  gbm5000 <- train(classe~., data = s5000, method = "gbm", verbose = FALSE)
```


We let the caret package run cross-validation and splits and automatically select the best tuning parameters. 


```{r chunkResults, cache = TRUE, autodep=TRUE}
  rf5000$finalModel
  gbm5000
```

The model print-outs estimate an out-of sample error rate. The out-of-bag error rate for the random forest model is 0.46%, calculated through cross-validation. This is slighlty higher than the best error estimation rate for boosting (0.26%).

We make a confusion matrix for the holdout set of data that was not used in training the data:

```{r chunkCM, cache = TRUE, autodep=TRUE}
  holdout = training[-c(as.numeric(row.names(s5000))),]
  pred <- predict(gbm5000, holdout)
  holdout$predRight <- pred == holdout$classe
  (t = table(pred, holdout$classe))
```

We calculate the accuracy of the GBM on this holdout set: 

```{r chunkAccuracy, cache = TRUE, autodep=TRUE}
  sum(diag(t))/nrow(holdout)
```

This performance is even slightly better than the out-of-sample error.

## Predictions
```{r chunkPred, cache = TRUE, autodep=TRUE}
  predA <- predict(rf5000, testing)
  predB <- predict(gbm5000, testing)
  data.frame(RF = predA, GBM = predB)
```

Model predictions agree with another.

```{r chunkExport, cache = TRUE, autodep=TRUE, echo=FALSE}
  pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
      filename = paste0("problem_id_",i,".txt")
      write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
      }
  }
  pml_write_files(predB)
```